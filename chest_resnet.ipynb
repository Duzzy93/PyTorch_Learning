{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uBVdg0Z5798H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24547,
     "status": "ok",
     "timestamp": 1751607436094,
     "user": {
      "displayName": "성윤석 (뚤더지)",
      "userId": "17565855815115520192"
     },
     "user_tz": -540
    },
    "id": "uBVdg0Z5798H",
    "outputId": "eaadbc6d-cfc2-4571-bb31-dcf22503187b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hU1IJQ079f3o",
   "metadata": {
    "id": "hU1IJQ079f3o"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U opencv-python\n",
    "!pip install -q -U albumentations\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5d762",
   "metadata": {
    "id": "2ea5d762"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision.datasets import ImageFolder\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision.models as models\n",
    "from torchvision.models.resnet import ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fa6a8",
   "metadata": {
    "id": "551fa6a8",
    "outputId": "acdf54d7-68ed-46a7-80e6-870b354b2804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False cpu\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "print(is_cuda, device)\n",
    "\n",
    "seed = 1010\n",
    "random.seed(seed)         # python seed\n",
    "np.random.seed(seed)      # numpy seed\n",
    "torch.manual_seed(seed)   # torch seed\n",
    "if device == 'cuda':\n",
    "  torch.cuda.manual_seed_all(seed)  # gpu seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e154d",
   "metadata": {
    "id": "475e154d"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/20250704/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r9ADm6IS80CP",
   "metadata": {
    "id": "r9ADm6IS80CP"
   },
   "outputs": [],
   "source": [
    "!unzip -qq -n \"Chest X-Ray Pneumonia.zip\" -d /content/drive/MyDrive/20250704/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aq-85H3i8-Pf",
   "metadata": {
    "id": "aq-85H3i8-Pf"
   },
   "outputs": [],
   "source": [
    "%ls /content/drive/MyDrive/20250704/dataset/chest_xray/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bPP8AU519Bkd",
   "metadata": {
    "id": "bPP8AU519Bkd"
   },
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "print(\"NORMAL:\", len(os.listdir(\"/content/drive/MyDrive/20250704/dataset/chest_xray/train/NORMAL\")), end=', ')\n",
    "print(\"PNEUMONIA:\", len(os.listdir(\"/content/drive/MyDrive/20250704/dataset/chest_xray/train/PNEUMONIA\")))\n",
    "print()\n",
    "print(\"Validation\")\n",
    "print(\"NORMAL:\", len(os.listdir(\"/content/drive/MyDrive/20250704/dataset/chest_xray/val/NORMAL\")), end=', ')\n",
    "print(\"PNEUMONIA:\", len(os.listdir(\"/content/drive/MyDrive/20250704/dataset/chest_xray/val/PNEUMONIA\")))\n",
    "print()\n",
    "print(\"Test\")\n",
    "print(\"NORMAL:\", len(os.listdir(\"/content/drive/MyDrive/20250704/dataset/chest_xray/test/NORMAL\")), end=', ')\n",
    "print(\"PNEUMONIA:\", len(os.listdir(\"/content/drive/MyDrive/20250704/dataset/chest_xray/test/PNEUMONIA\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pDNTH4KK9FJt",
   "metadata": {
    "id": "pDNTH4KK9FJt"
   },
   "outputs": [],
   "source": [
    "root = \"/content/drive/MyDrive/20250704/dataset/chest_xray/test/\"\n",
    "normal_dir = root + 'NORMAL/'\n",
    "pneumonia_dir = root + 'PNEUMONIA/'\n",
    "\n",
    "normal = list(map(lambda x: normal_dir+x, os.listdir(normal_dir)[:5]))\n",
    "pneumonia = list(map(lambda x: pneumonia_dir+x, os.listdir(pneumonia_dir)[:5]))\n",
    "\n",
    "samples = pneumonia + normal\n",
    "\n",
    "# show samples\n",
    "plt.figure(figsize=(30,10))\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i+1)\n",
    "  img = Image.open(samples[i])\n",
    "\n",
    "  ax = plt.gca()\n",
    "  ax.set_title(\"Pneumonia\" if i < 5 else \"Normal\")\n",
    "  ax.imshow(img, cmap='gray')\n",
    "  ax.axis('off')\n",
    "  ax.set_aspect('auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VerX0z8C9TtI",
   "metadata": {
    "id": "VerX0z8C9TtI"
   },
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    root=\"/content/drive/MyDrive/20250704/dataset/chest_xray\", val=0.1,\n",
    "    train_transforms=None, test_transforms=None\n",
    "):\n",
    "  origin = datasets.ImageFolder(\n",
    "      os.path.join(root, 'train'),\n",
    "      transform=train_transforms\n",
    "  )\n",
    "\n",
    "  val_samples = int(len(origin) * val)\n",
    "  train_samples = len(origin) - val_samples\n",
    "\n",
    "  trainset, valset = torch.utils.data.random_split(\n",
    "    origin,\n",
    "    (train_samples, val_samples),\n",
    "  )\n",
    "  valset.transforms = test_transforms\n",
    "\n",
    "  testset = datasets.ImageFolder(\n",
    "      os.path.join(root, 'test'),\n",
    "      transform=test_transforms\n",
    "  )\n",
    "  return trainset, valset, testset\n",
    "\n",
    "trainset, valset, testset = get_dataset(train_transforms=transforms.ToTensor())\n",
    "print(len(trainset), len(valset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319kEcN9Xf8",
   "metadata": {
    "id": "3319kEcN9Xf8"
   },
   "outputs": [],
   "source": [
    "class_names = trainset.dataset.classes\n",
    "print(class_names)\n",
    "print(trainset.dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918vplNp9ab8",
   "metadata": {
    "id": "918vplNp9ab8"
   },
   "outputs": [],
   "source": [
    "image, label = trainset[0]\n",
    "\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lRUNa1Xa9cVM",
   "metadata": {
    "id": "lRUNa1Xa9cVM"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomRotation(degrees=(-20,+20)),\n",
    "        transforms.RandomCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "trainset, _, _ = get_dataset(\n",
    "  train_transforms=train_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "  dataset=trainset,\n",
    "  shuffle=True,\n",
    "  batch_size=64,\n",
    "  num_workers=0,\n",
    ")\n",
    "\n",
    "for i in tqdm(train_loader):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L37aRpY4-JVF",
   "metadata": {
    "id": "L37aRpY4-JVF"
   },
   "outputs": [],
   "source": [
    "class AlbumentationsDataset(ImageFolder):\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    path, target = self.samples[index]\n",
    "    # Read image\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Transform\n",
    "    if self.transform is not None:\n",
    "      augmented = self.transform(image=image)\n",
    "      image = augmented['image']\n",
    "\n",
    "    return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-5DzziYq-Mcb",
   "metadata": {
    "id": "-5DzziYq-Mcb"
   },
   "outputs": [],
   "source": [
    "def get_dataset_v2(root=\"/content/drive/MyDrive/20250704/dataset/chest_xray\", val=0.1, train_transforms=None, test_transforms=None):\n",
    "  origin = AlbumentationsDataset(os.path.join(root, 'train'), transform=train_transforms)\n",
    "\n",
    "  val_samples = int(len(origin) * val)\n",
    "  train_samples = len(origin) - val_samples\n",
    "\n",
    "  trainset, valset = torch.utils.data.random_split(\n",
    "    origin,\n",
    "    (train_samples, val_samples),\n",
    "  )\n",
    "  valset.transforms = test_transforms\n",
    "\n",
    "  testset = AlbumentationsDataset(os.path.join(root, 'test'), transform=test_transforms)\n",
    "  return trainset, valset, testset\n",
    "\n",
    "trainset, valset, testset = get_dataset_v2(train_transforms=None)\n",
    "print(len(trainset), len(valset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZXjnEniH-NF7",
   "metadata": {
    "id": "ZXjnEniH-NF7"
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Rotate(limit=(-20, +20)),\n",
    "        A.RandomCrop(224, 224),\n",
    "        A.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "])\n",
    "\n",
    "trainset, _, _ = get_dataset_v2(\n",
    "  train_transforms=train_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "  dataset=trainset,\n",
    "  shuffle=True,\n",
    "  batch_size=64,\n",
    "  num_workers=0,\n",
    ")\n",
    "\n",
    "for i in tqdm(train_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YxW9z6st-Tqb",
   "metadata": {
    "id": "YxW9z6st-Tqb"
   },
   "outputs": [],
   "source": [
    "test_transform = A.Compose([\n",
    "        A.Resize(256,256),\n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(p=1),\n",
    "            A.RandomRotate90(p=1),\n",
    "            A.VerticalFlip(p=1)\n",
    "        ], p=1),\n",
    "        A.CenterCrop(224, 224),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.3),\n",
    "            A.OpticalDistortion(p=0.4),\n",
    "            A.GaussNoise(p=0.5)\n",
    "        ], p=0.5),\n",
    "        ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FjDgOjEq-Vr7",
   "metadata": {
    "id": "FjDgOjEq-Vr7"
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"/content/drive/MyDrive/20250704/dataset/chest_xray/\")\n",
    "testset = AlbumentationsDataset(data_dir / 'test', transform=test_transform)\n",
    "\n",
    "num_samples = 5\n",
    "fig, ax = plt.subplots(1, num_samples, figsize=(25, 5))\n",
    "for i in range(num_samples):\n",
    "  ax[i].imshow(transforms.ToPILImage()(testset[0][0]))\n",
    "  ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1jSNnw-siN",
   "metadata": {
    "id": "7f1jSNnw-siN"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rua_ANmN-vyK",
   "metadata": {
    "id": "Rua_ANmN-vyK"
   },
   "outputs": [],
   "source": [
    "def load_resnet():\n",
    "  model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "  model.fc = nn.Linear(in_features=512, out_features=1, bias=True)  ###\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1i4nXSg-yB6",
   "metadata": {
    "id": "p1i4nXSg-yB6"
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hl2qrLwC_SUx",
   "metadata": {
    "id": "hl2qrLwC_SUx"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epoch, device):\n",
    "  # train mode\n",
    "  model.train()\n",
    "\n",
    "  # 학습 통계\n",
    "  running_loss = 0\n",
    "  correct = 0\n",
    "\n",
    "  with tqdm(dataloader) as pbar:\n",
    "    for i, (data, targets) in enumerate(pbar):\n",
    "      data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs, targets.unsqueeze(1).float()) #### Change\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "      pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "      # Accuracy 계산\n",
    "      with torch.no_grad():\n",
    "        predicted = torch.sigmoid(outputs).round() #### Change\n",
    "        correct += predicted.eq(targets.view_as(predicted)).sum()\n",
    "\n",
    "    # Accuracy 출력\n",
    "    data_num = len(dataloader.dataset)\n",
    "    acc = 100. * correct / data_num\n",
    "    print(\n",
    "        f\"[{epoch}/{EPOCH}]\",\n",
    "        f\"train loss: {running_loss/len(dataloader):.4f}\",\n",
    "        f\"train acc: {correct}/{data_num} ({acc:.2f}%)\"\n",
    "    )\n",
    "\n",
    "  return running_loss/len(dataloader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ux9hpQWz_WgZ",
   "metadata": {
    "id": "Ux9hpQWz_WgZ"
   },
   "outputs": [],
   "source": [
    "def validation(model, dataloader, criterion, epoch, device):\n",
    "  # eval 모드\n",
    "  model.eval()\n",
    "\n",
    "  # 검증 통계\n",
    "  correct = 0\n",
    "  running_loss = 0.\n",
    "\n",
    "  with tqdm(dataloader) as pbar:\n",
    "    with torch.no_grad():\n",
    "      for i, (data, targets) in enumerate(pbar):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1).float())  #### Change\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Accuracy 계산\n",
    "        predicted = torch.sigmoid(outputs).round() #### Change\n",
    "        correct += predicted.eq(targets.view_as(predicted)).sum()\n",
    "\n",
    "  # Accuracy 계산\n",
    "  data_num = len(dataloader.dataset)\n",
    "  acc = 100. * correct / data_num\n",
    "  print(f'[{epoch}/{EPOCH}] valid loss: {running_loss/len(dataloader):.4f} valid acc: {correct}/{data_num} ({acc:.2f}%)\\n')\n",
    "\n",
    "  return running_loss/len(dataloader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bJzNXKIr_Z5B",
   "metadata": {
    "id": "bJzNXKIr_Z5B"
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader, device):\n",
    "    # eval 모드\n",
    "    model.eval()\n",
    "\n",
    "    # 테스트 통계\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for data, targets in dataloader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(data)    # forward\n",
    "\n",
    "        # Accuracy 계산\n",
    "        predicted = torch.sigmoid(outputs).round()  ### Change\n",
    "        correct += predicted.eq(targets.view_as(predicted)).sum()\n",
    "\n",
    "        y_true.append(targets)\n",
    "        y_pred.append(outputs)\n",
    "\n",
    "    # Accuracy 계산\n",
    "    data_num = len(dataloader.dataset)\n",
    "    print(f'Test Accuracy: {correct}/{data_num} ({100. * correct / data_num:.2f}%)')\n",
    "\n",
    "    return 100. * correct / data_num, torch.cat(y_true), torch.cat(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_0HVQo3q_cqb",
   "metadata": {
    "id": "_0HVQo3q_cqb"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"Pneumonia\", save_code=True)\n",
    "\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 0\n",
    "LR = 0.001\n",
    "\n",
    "wandb.config = {\n",
    "  \"learning_rate\": LR,\n",
    "  \"epochs\": EPOCH,\n",
    "  \"batch_size\": BATCH_SIZE,\n",
    "  \"num_workers\": NUM_WORKERS\n",
    "}\n",
    "\n",
    "train_transform = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Rotate(limit=(-20, +20)),\n",
    "        A.RandomCrop(224, 224),\n",
    "        A.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "])\n",
    "test_transform = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.CenterCrop(224, 224),\n",
    "        A.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "])\n",
    "\n",
    "trainset, valset, testset = get_dataset_v2(\n",
    "  train_transforms=train_transform,\n",
    "  test_transforms=test_transform\n",
    ")\n",
    "\n",
    "# dataloader\n",
    "train_loader = DataLoader(\n",
    "  dataset=trainset,\n",
    "  shuffle=True,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  num_workers=NUM_WORKERS,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "  dataset=valset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  num_workers=NUM_WORKERS,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "  dataset=testset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = load_resnet()\n",
    "\n",
    "# Optimizer, Loss, Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "max_acc = 0\n",
    "# Start Training\n",
    "for epoch in range(EPOCH):\n",
    "  print(\"LR:\", scheduler.get_last_lr())\n",
    "\n",
    "  tloss, tacc = train(model, train_loader, criterion, optimizer, epoch, device)\n",
    "  vloss, vacc = validation(model, val_loader, criterion, epoch, device)\n",
    "\n",
    "  wandb.log({\n",
    "      \"lr\": scheduler.get_last_lr()[0],\n",
    "      \"train_loss\": tloss,\n",
    "      \"train_accuracy\": tacc,\n",
    "      \"val_loss\": vloss,\n",
    "      \"val_acc\": vacc\n",
    "  })\n",
    "  scheduler.step()\n",
    "\n",
    "  if vacc > max_acc:\n",
    "    torch.save(model.state_dict(), \"best.pth\")\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(torch.load(\"best.pth\"))\n",
    "artifact = wandb.Artifact('best', type='checkpoint')\n",
    "artifact.add_file('best.pth')\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "# Test\n",
    "tacc, y_true, y_preds = test(model, test_loader, device)\n",
    "class_names = testset.classes\n",
    "wandb.log({\n",
    "  \"test_accuracy\": tacc,\n",
    "  \"conf_mat\": wandb.plot.confusion_matrix(probs=None,\n",
    "                y_true=y_true.tolist(),\n",
    "                preds=torch.sigmoid(y_preds).squeeze().round().int().tolist(),\n",
    "                class_names=class_names)})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
